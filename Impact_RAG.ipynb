{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f9eb16",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Colab asks to restart after installing, accept it.\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"sentence-transformers>=3.0.1\" \\\n",
    "                 \"faiss-cpu>=1.8.0\" \"langchain-text-splitters>=0.3.0\" \"pypdf>=4.2.0\" \\\n",
    "                 \"gradio>=4.44.0\"\n",
    "# bitsandbytes is optional (for 8-bit loading if a GPU is available). It may fail on CPU-only.\n",
    "!pip -q install bitsandbytes==0.44.1 || echo \"bitsandbytes optional install skipped\"\n",
    "!pip -q install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60541112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Retrieve secret\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Export it so the rest of the notebook sees it\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "print(\"Gemini key loaded:\", \"‚úÖ\" if GOOGLE_API_KEY else \"‚ùå Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86c0a9",
   "metadata": {},
   "source": [
    "CONFIGURE MODEL BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536100a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Choose your generator backend ===\n",
    "# \"local\" uses a small open-source chat model via Hugging Face Transformers.\n",
    "# \"openai\" uses OpenAI's API (set OPENAI_API_KEY).\n",
    "# \"gemini\" uses Google Generative AI (set GOOGLE_API_KEY).\n",
    "GEN_BACKEND = \"gemini\"  # options: \"local\", \"openai\", \"gemini\"\n",
    "\n",
    "# Local model config:\n",
    "LOCAL_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # light model for CPU/GPU demos\n",
    "\n",
    "# Retrieval config\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 4\n",
    "\n",
    "# Prompt template\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. Use the provided context to answer the user's question.\\n\"\n",
    "    \"If the answer is not in the context, say you don't know.\\n\"\n",
    ")\n",
    "\n",
    "ANSWER_TEMPLATE = \"\"\"[System]\n",
    "{system}\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[Instructions]\n",
    "- Cite the most relevant chunks briefly (e.g., 'From chunk 2').\n",
    "- If unsure, say 'I don't know from the provided docs.'\n",
    "- Keep answers concise and factual.\n",
    "\"\"\"\n",
    "\n",
    "OPENAI_API_KEY = globals().get(\"OPENAI_API_KEY\", os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "GOOGLE_API_KEY = globals().get(\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd789960",
   "metadata": {},
   "source": [
    "LOAD FILES/CHUNKS AND EMBEDDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_texts_from_paths(paths: List[str]) -> List[Dict]:\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if p.lower().endswith(\".pdf\"):\n",
    "            text = \"\"\n",
    "            try:\n",
    "                reader = PdfReader(p)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to parse PDF {p}: {e}\")\n",
    "                continue\n",
    "        elif p.lower().endswith((\".txt\",\".md\")):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print(f\"[SKIP] Unsupported file type: {p}\")\n",
    "            continue\n",
    "        docs.append({\"path\": p, \"text\": text})\n",
    "    return docs\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len\n",
    ")\n",
    "\n",
    "def chunk_docs(docs: List[Dict]) -> List[Dict]:\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        for i, ch in enumerate(splitter.split_text(d[\"text\"])):\n",
    "            chunks.append({\"source\": d[\"path\"], \"chunk_id\": i, \"text\": ch})\n",
    "    return chunks\n",
    "\n",
    "class RAGIndex:\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.model = SentenceTransformer(embedding_model_name, device=device)\n",
    "        self.index = None\n",
    "        self.chunks: List[Dict] = []\n",
    "\n",
    "    def build(self, chunks: List[Dict]):\n",
    "        self.chunks = chunks\n",
    "        embs = self.model.encode([c[\"text\"] for c in chunks], convert_to_numpy=True, show_progress_bar=True)\n",
    "        dim = embs.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        faiss.normalize_L2(embs)\n",
    "        index.add(embs)\n",
    "        self.index = index\n",
    "        print(f\"Built index with {len(chunks)} chunks.\")\n",
    "\n",
    "    def search(self, query: str, k: int = 4):\n",
    "        if self.index is None or not self.chunks:\n",
    "            return []\n",
    "        q = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q)\n",
    "        scores, idxs = self.index.search(q, k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            if idx == -1: continue\n",
    "            results.append((float(score), self.chunks[idx]))\n",
    "        return results\n",
    "\n",
    "rag = RAGIndex(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0be20",
   "metadata": {},
   "source": [
    "GENERATIONS BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99755ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_context(snippets):\n",
    "    lines = []\n",
    "    for rank, (score, ch) in enumerate(snippets, start=1):\n",
    "        header = f\"[Chunk {rank}] (score={score:.3f}) source={os.path.basename(ch['source'])} id={ch['chunk_id']}\"\n",
    "        lines.append(header + \"\\n\" + ch[\"text\"])\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "def build_prompt(question, context_blocks):\n",
    "    return ANSWER_TEMPLATE.format(\n",
    "        system=SYSTEM_PROMPT.strip(),\n",
    "        context=context_blocks.strip(),\n",
    "        question=question.strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6862f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "_local_pipe = None\n",
    "\n",
    "def get_local_pipe():\n",
    "    global _local_pipe\n",
    "    if _local_pipe is None:\n",
    "        tok = AutoTokenizer.from_pretrained(LOCAL_MODEL, use_fast=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LOCAL_MODEL,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        _local_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tok,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_new_tokens=384,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.05\n",
    "        )\n",
    "    return _local_pipe\n",
    "\n",
    "def generate_local(prompt: str) -> str:\n",
    "    p = get_local_pipe()\n",
    "    out = p(prompt, pad_token_id=p.tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "    return out[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac96e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai(prompt: str) -> str:\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"OPENAI_API_KEY not set. Switch GEN_BACKEND to 'local' or set your key.\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        r = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        return r.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[OpenAI error] {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6348d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini(prompt: str) -> str:\n",
    "    if not GOOGLE_API_KEY:\n",
    "        return \"GOOGLE_API_KEY not set. Switch GEN_BACKEND to 'local' or set your key.\"\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "        r = model.generate_content(prompt)\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        return f\"[Gemini error] {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cf8ec",
   "metadata": {},
   "source": [
    "ASK QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, top_k: int = TOP_K):\n",
    "    hits = rag.search(question, k=top_k)\n",
    "    context = render_context(hits)\n",
    "    prompt = build_prompt(question, context)\n",
    "\n",
    "    if GEN_BACKEND == \"local\":\n",
    "        answer = generate_local(prompt)\n",
    "    elif GEN_BACKEND == \"openai\":\n",
    "        answer = generate_openai(prompt)\n",
    "    elif GEN_BACKEND == \"gemini\":\n",
    "        answer = generate_gemini(prompt)\n",
    "    else:\n",
    "        answer = f\"Unknown backend: {GEN_BACKEND}\"\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"top_chunks\": hits}\n",
    "\n",
    "print(\"RAG ready. After indexing, call: answer_question('Your query')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai, os\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "available = [m.name for m in genai.list_models()\n",
    "             if \"generateContent\" in getattr(m, \"supported_generation_methods\", [])]\n",
    "for name in available:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai, time, os\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "m = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "t=time.time()\n",
    "print(m.generate_content(\"Say only: ready\").text, \"‚è±\", round(time.time()-t,2), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29c428",
   "metadata": {},
   "source": [
    "UPLOAD FILESAND BUILD INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "def upload_files():\n",
    "    print(\"Select PDFs/TXT/MD files...\")\n",
    "    uploaded = files.upload()\n",
    "    paths = []\n",
    "    for name, data in uploaded.items():\n",
    "        path = f\"/content/{name}\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def build_index_from_paths(paths):\n",
    "    docs = load_texts_from_paths(paths)\n",
    "    chunks = chunk_docs(docs)\n",
    "    rag.build(chunks)\n",
    "    print(f\"Indexed {len(chunks)} chunks from {len(docs)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ccc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Pick your PDFs / TXT / MD from your computer\n",
    "paths = upload_files()                 # opens a file picker\n",
    "\n",
    "# 2) Build the index (chunk + embed + FAISS)\n",
    "build_index_from_paths(paths)\n",
    "\n",
    "# 3) Sanity checks\n",
    "print(\"Files:\", paths)\n",
    "print(\"Chunks indexed:\", len(rag.chunks))\n",
    "print(\"First chunk preview:\\n\", rag.chunks[0][\"text\"][:500] if rag.chunks else \"No chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70661457",
   "metadata": {},
   "source": [
    "GRADIO CHAT OVERLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71da932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "def read_text_file_to_string(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a text file from a filepath and returns its contents as a string.\n",
    "    \"\"\"\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def _gradio_story_eval(story_text: str, mission: str = \"\"):\n",
    "    \"\"\"\n",
    "    Runs an Impact-Studios-style evaluation, but grounded in your RAG index\n",
    "    when available.\n",
    "    \"\"\"\n",
    "    if not story_text.strip():\n",
    "        return \"‚ö†Ô∏è Please enter a valid story, concept, or idea.\"\n",
    "\n",
    "    # --- Build the creative directive prompt ---\n",
    "    base_prompt = \"\"\"\n",
    "    You are an editorial advisor focused on community engagement and ethical publication. Read the following piece of writing\n",
    "    and identify the communities, audiences, or individuals who may be directly affected by its themes.Then provide specific,\n",
    "    actionable suggestions for how the author can responsibly and meaningfully reach out to or\n",
    "    support those communities through or alongside the text.\n",
    "\n",
    "    Your feedback must:\n",
    "    -Be concrete (e.g., exact additions, placements, wording ideas, or resources to include).\n",
    "    -Explain why each suggestion is appropriate for the content.\n",
    "    -Address ethical considerations such as harm reduction, accessibility, and care for vulnerable readers when relevant.\n",
    "\n",
    "    Avoid generic advice like ‚Äúbe sensitive‚Äù or ‚Äúraise awareness.‚Äù\n",
    "\n",
    "    When recommending a resource, make sure to provide a way to access it such as a phone number or website.\n",
    "\n",
    "    Do not suggest any changes to the origional content like \"consider altering the ending\" or \"elaborate on this\".\n",
    "\n",
    "    When users share a script, concept, or idea, your job is to:\n",
    "    1. Analyze the submission‚Äôs tone, themes, and emotional depth.\n",
    "    2. Determine the topics that are discussed in the submission.\n",
    "    3. Provide clear reasoning for your evaluation.\n",
    "    4. Provide specific, actionable suggestions for how the author can responsibly and meaningfully reach out to or support those communities through or alongside the text.\n",
    "    5. If any information is provided, integrate it naturally into your evaluation.\n",
    "\n",
    "    Respond in this format:\n",
    "\n",
    "    Topics identified:\n",
    "    (List of topics identified in the submission. Each topic should be listed with at least one quote from the submission that exemplifies the listed topic and at least one specific, actionable recommendations for outreach and support actions)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Combine with the optional mission ---\n",
    "    if mission and mission.strip():\n",
    "        user_prompt = f\"{base_prompt}\\n\\nCurrent Studio Mission:\\n{mission}\\n\\nUser Submission:\\n{story_text}\"\n",
    "    else:\n",
    "        user_prompt = f\"{base_prompt}\\n\\nUser Submission:\\n{story_text}\"\n",
    "\n",
    "    print(\"Using backend:\", GEN_BACKEND)\n",
    "\n",
    "    # --- If you have RAG docs, retrieve and inject context ---\n",
    "    context_text = \"\"\n",
    "    if \"rag\" in globals() and getattr(rag, \"chunks\", []):\n",
    "        hits = rag.search(story_text, k=TOP_K)\n",
    "        context_text = render_context(hits)\n",
    "        print(f\"Injected {len(hits)} RAG chunks as grounding context.\")\n",
    "    else:\n",
    "        print(\"No RAG index loaded ‚Äî using prompt only.\")\n",
    "\n",
    "    # --- Build full combined prompt for Gemini/OpenAI/local ---\n",
    "    full_prompt = (\n",
    "        ANSWER_TEMPLATE.format(\n",
    "            system=SYSTEM_PROMPT,\n",
    "            context=context_text or \"[No extra context]\",\n",
    "            question=user_prompt,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    out = answer_question(full_prompt)\n",
    "    answer = out[\"answer\"]\n",
    "\n",
    "    # --- Append citation summary if RAG used ---\n",
    "    cites = []\n",
    "    if context_text:\n",
    "        for rank, (_, ch) in enumerate(out[\"top_chunks\"], start=1):\n",
    "            cites.append(f\"Chunk {rank} ‚Äî {os.path.basename(ch['source'])}#{ch['chunk_id']}\")\n",
    "    suffix = (\"\\n\\n---\\nSources: \" + \"; \".join(cites)) if cites else \"\"\n",
    "\n",
    "    return (answer or \"[Empty answer]\") + suffix\n",
    "\n",
    "\n",
    "CUSTOM_CSS = \"\"\"\n",
    "body, .gradio-container {\n",
    "    background-color: rgb(106, 179, 81) !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# === Gradio UI ===\n",
    "with gr.Blocks(css=CUSTOM_CSS) as demo:\n",
    "    gr.Markdown(\"## üé¨ Impact Studios RAG Evaluator\")\n",
    "    gr.Markdown(\n",
    "        \"Analyze stories or concepts to see how well they uplift humanity ‚Äî \"\n",
    "        \"grounded in your uploaded reference docs if available.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        # LEFT COLUMN ‚Äî uploader\n",
    "        with gr.Column(scale=4):\n",
    "            uploaded_txt = gr.File(\n",
    "                label=\"Script Upload\",\n",
    "                file_types=[\".txt\"],\n",
    "                type=\"filepath\",\n",
    "                height=160,\n",
    "            )\n",
    "\n",
    "        # RIGHT COLUMN ‚Äî input\n",
    "        with gr.Column(scale=4):\n",
    "            story = gr.Textbox(\n",
    "                label=\"Enter your story, script, or concept\",\n",
    "                placeholder=\"Example: A young girl follows a rabbit into a strange world that challenges her perception of reality...\",\n",
    "                lines=8,\n",
    "            )\n",
    "\n",
    "    output = gr.Textbox(label=\"Impact Evaluation\", lines=12)\n",
    "    btn = gr.Button(\"Evaluate\")\n",
    "\n",
    "    uploaded_txt.change(\n",
    "        fn=read_text_file_to_string,\n",
    "        inputs=uploaded_txt,\n",
    "        outputs=story,\n",
    "    )\n",
    "\n",
    "    btn.click(_gradio_story_eval, inputs=[story], outputs=output)\n",
    "\n",
    "print(\"Launching Impact Studios Evaluator‚Ä¶\")\n",
    "demo.launch(share=True, debug=True)\n",
    "\n",
    "btn.click(_gradio_story_eval, inputs=[story], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this if first code block fails\n",
    "import os, gradio as gr\n",
    "\n",
    "def _gradio_ask(q: str):\n",
    "    if not q.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    print(\"Using backend:\", GEN_BACKEND)  # shows in Colab logs\n",
    "    out = answer_question(q)\n",
    "    cites = []\n",
    "    for rank, (_, ch) in enumerate(out[\"top_chunks\"], start=1):\n",
    "        cites.append(f\"Chunk {rank} ‚Äî {os.path.basename(ch['source'])}#{ch['chunk_id']}\")\n",
    "    suffix = (\"\\n\\n---\\nSources: \" + \"; \".join(cites)) if cites else \"\"\n",
    "    return (out[\"answer\"] or \"[Empty answer]\") + suffix\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### RAG Chat ‚Äî ask questions grounded in your uploaded docs\")\n",
    "    q = gr.Textbox(label=\"Your question\")\n",
    "    a = gr.Markdown(label=\"Answer\")\n",
    "    btn = gr.Button(\"Ask\")\n",
    "    btn.click(_gradio_ask, inputs=q, outputs=a)\n",
    "\n",
    "print(\"Launching RAG chat‚Ä¶\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
